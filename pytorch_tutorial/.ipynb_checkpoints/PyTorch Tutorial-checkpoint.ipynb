{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a tensor from a Python List\n",
    "data = [\n",
    "    [0, 1],\n",
    "    [2, 3],\n",
    "    [4, 5]\n",
    "]\n",
    "x_python = torch.tensor(data)\n",
    "\n",
    "# Print the tensor\n",
    "x_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_float = torch.tensor(data, dtype=torch.float)\n",
    "x_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True],\n",
       "        [ True,  True],\n",
       "        [ True,  True]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bool = torch.tensor(data, dtype=torch.bool)\n",
    "x_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1.],\n",
       "        [2., 3.],\n",
       "        [4., 5.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_python.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ndarray = np.array(data)\n",
    "x_numpy = torch.from_numpy(ndarray)\n",
    "\n",
    "x_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From a Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base tensor\n",
    "x = torch.tensor([[1., 2.], [3., 4.]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_zeros = torch.zeros_like(x)\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ones = torch.ones_like(x)\n",
    "x_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8962, 0.6999],\n",
       "        [0.1680, 0.3985]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rand = torch.rand_like(x)\n",
    "x_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0109,  0.3549],\n",
       "        [-1.6442, -0.0284]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_randn = torch.randn_like(x)\n",
    "x_randn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## By specifying a Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = (4, 2, 2)\n",
    "x_zeros = torch.zeros(shape)\n",
    "x_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, 2)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_view = x.view(3, 2)\n",
    "x_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_view = x.view(-1, 3)\n",
    "x_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = torch.reshape(x, (2, 3))\n",
    "x_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [2, 3],\n",
       "        [4, 5],\n",
       "        [6, 7],\n",
       "        [8, 9]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(10).reshape(5, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.unsqueeze(1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.squeeze()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2], [3, 4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an example tensor\n",
    "x = torch.Tensor([\n",
    "                  [[1, 2], [3, 4]],\n",
    "                  [[5, 6], [7, 8]], \n",
    "                  [[9, 10], [11, 12]] \n",
    "                 ])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  2.],\n",
       "         [ 3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.],\n",
       "         [ 7.,  8.]],\n",
       "\n",
       "        [[ 9., 10.],\n",
       "         [11., 12.]]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[1., 2.],\n",
       "         [3., 4.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]],\n",
       "\n",
       "        [[5., 6.],\n",
       "         [7., 8.]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([0, 0, 1, 1])\n",
    "x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.,  6.],\n",
       "        [ 9., 10.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = torch.tensor([1, 2])\n",
    "j = torch.tensor([0])\n",
    "x[i, j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0, 0, 0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]],\n",
       "\n",
       "        [[1., 1.],\n",
       "         [1., 1.]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones(3, 2, 2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[3., 3.],\n",
       "         [3., 3.]],\n",
       "\n",
       "        [[3., 3.],\n",
       "         [3., 3.]]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.]],\n",
       "\n",
       "        [[2., 2.],\n",
       "         [2., 2.]]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((4, 3)) * 6\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(3) * 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.],\n",
       "        [3., 3., 3.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36., 36., 36., 36.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a @ b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(a.shape)\n",
    "pp.pprint(a.T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Mean: 2.5'\n",
      "'Mean in the 0th dimension: tensor([2.5000, 2.5000])'\n",
      "'Mean in the 1th dimension: tensor([1., 2., 3., 4.])'\n"
     ]
    }
   ],
   "source": [
    "# Create an example tensor\n",
    "m = torch.tensor(\n",
    "    [\n",
    "     [1., 1.],\n",
    "     [2., 2.],\n",
    "     [3., 3.],\n",
    "     [4., 4.]\n",
    "    ]\n",
    ")\n",
    "\n",
    "pp.pprint(\"Mean: {}\".format(m.mean()))\n",
    "pp.pprint(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n",
    "pp.pprint(\"Mean in the 1th dimension: {}\".format(m.mean(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: torch.Size([4, 3])\n",
      "Shape after concatenation in dimension 0: torch.Size([12, 3])\n",
      "Shape after concatenation in dimension 1: torch.Size([4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Concatenate in dimension 0 and 1\n",
    "a_cat0 = torch.cat([a, a, a], dim=0)\n",
    "a_cat1 = torch.cat([a, a, a], dim=1)\n",
    "\n",
    "print(\"Initial shape: {}\".format(a.shape))\n",
    "print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n",
    "print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.],\n",
       "        [6., 6., 6.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12.],\n",
       "        [12., 12., 12.],\n",
       "        [12., 12., 12.],\n",
       "        [12., 12., 12.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.add_(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "y = x * x * 3\n",
    "y.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.])\n"
     ]
    }
   ],
   "source": [
    "z = x * x *3\n",
    "z.backward()\n",
    "pp.pprint(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957]],\n",
       "\n",
       "        [[-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.ones(2, 3, 4)\n",
    "\n",
    "linear = nn.Linear(4, 2)\n",
    "linear_output = linear(input)\n",
    "linear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957]],\n",
       "\n",
       "        [[-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957],\n",
       "         [-0.1600, -0.2957]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4601, 0.4266],\n",
       "         [0.4601, 0.4266],\n",
       "         [0.4601, 0.4266]],\n",
       "\n",
       "        [[0.4601, 0.4266],\n",
       "         [0.4601, 0.4266],\n",
       "         [0.4601, 0.4266]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid = nn.Sigmoid()\n",
    "output = sigmoid(linear_output)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the Layers Together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.4533, 0.7074],\n",
       "         [0.4533, 0.7074],\n",
       "         [0.4533, 0.7074]],\n",
       "\n",
       "        [[0.4533, 0.7074],\n",
       "         [0.4533, 0.7074],\n",
       "         [0.4533, 0.7074]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = nn.Sequential(\n",
    "    nn.Linear(4, 2),\n",
    "    nn.Sigmoid())\n",
    "\n",
    "input = torch.ones(2, 3, 4)\n",
    "output = block(input)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Modulues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(MultilayerPerceptron, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(self.input_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.model(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7531,  2.1106, -0.7007, -0.0859, -0.1345],\n",
       "        [-1.8899,  0.3836,  0.9575, -0.7254, -1.1139]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(2, 5)\n",
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4950, 0.3568, 0.5792, 0.5409, 0.3772],\n",
       "        [0.5608, 0.3628, 0.6255, 0.5288, 0.3067]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(5, 3)\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('model.0.weight', Parameter containing:\n",
       "  tensor([[-0.2167,  0.3756,  0.4341, -0.3957, -0.4406],\n",
       "          [ 0.2018,  0.1818,  0.3157, -0.4461,  0.2801],\n",
       "          [-0.3928, -0.2104,  0.3142,  0.3246,  0.1554]], requires_grad=True)),\n",
       " ('model.0.bias', Parameter containing:\n",
       "  tensor([ 0.1612, -0.0521, -0.0284], requires_grad=True)),\n",
       " ('model.2.weight', Parameter containing:\n",
       "  tensor([[ 0.4369,  0.0720, -0.3297],\n",
       "          [-0.1795,  0.0237,  0.3918],\n",
       "          [-0.0059,  0.4957,  0.3796],\n",
       "          [-0.0787, -0.2435,  0.0574],\n",
       "          [-0.1284, -0.1452, -0.3530]], requires_grad=True)),\n",
       " ('model.2.bias', Parameter containing:\n",
       "  tensor([-0.4160, -0.4267,  0.3247,  0.2353, -0.3850], requires_grad=True))]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7779,  0.9647,  1.9476,  1.9706,  1.7899],\n",
       "        [-0.1213,  3.3108,  0.6291,  1.3773,  2.8998],\n",
       "        [ 2.4536, -0.8032,  0.8095,  1.4692, -0.0266],\n",
       "        [ 1.3653,  1.2021,  0.0934,  1.0497,  2.4420],\n",
       "        [-0.0276,  1.9113,  1.2256,  0.6083,  1.2179],\n",
       "        [ 2.2881,  2.1811,  0.7544,  1.6644,  0.9204],\n",
       "        [ 2.6070,  0.8955,  2.0630,  2.1397,  0.5987],\n",
       "        [ 2.7572,  1.0345,  0.0089, -0.3678,  1.8597],\n",
       "        [ 1.3556,  1.3756,  0.4785,  0.9865,  0.2422],\n",
       "        [ 1.9422,  1.7130,  2.2025,  1.6128,  0.9251]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.ones(10, 5)\n",
    "\n",
    "x = y + torch.randn_like(y)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8227662444114685"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultilayerPerceptron(5, 3)\n",
    "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "y_pred = model(x)\n",
    "loss_function(y_pred, y).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: training loss: 0.8227662444114685\n",
      "Epoch 1: training loss: 0.676514744758606\n",
      "Epoch 2: training loss: 0.5837696194648743\n",
      "Epoch 3: training loss: 0.4635762870311737\n",
      "Epoch 4: training loss: 0.32721611857414246\n",
      "Epoch 5: training loss: 0.1965080052614212\n",
      "Epoch 6: training loss: 0.09940735995769501\n",
      "Epoch 7: training loss: 0.04349245876073837\n",
      "Epoch 8: training loss: 0.017437821254134178\n",
      "Epoch 9: training loss: 0.006805609446018934\n"
     ]
    }
   ],
   "source": [
    "n_epoch = 10\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    adam.zero_grad()\n",
    "    y_pred = model(x)\n",
    "    loss = loss_function(y_pred, y)\n",
    "    print(f\"Epoch {epoch}: training loss: {loss}\")\n",
    "    loss.backward()\n",
    "    adam.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9988, 0.9996, 0.9999, 1.0000, 0.9983],\n",
       "        [0.9987, 0.9994, 0.9998, 1.0000, 0.9984],\n",
       "        [0.9880, 0.9950, 0.9978, 0.9993, 0.9857],\n",
       "        [0.9968, 0.9984, 0.9993, 0.9999, 0.9963],\n",
       "        [0.9912, 0.9956, 0.9974, 0.9995, 0.9904],\n",
       "        [0.9988, 0.9995, 0.9999, 1.0000, 0.9984],\n",
       "        [0.9994, 0.9998, 1.0000, 1.0000, 0.9991],\n",
       "        [0.9959, 0.9977, 0.9983, 0.9999, 0.9958],\n",
       "        [0.9869, 0.9937, 0.9963, 0.9990, 0.9857],\n",
       "        [0.9994, 0.9998, 1.0000, 1.0000, 0.9992]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model(x)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9993, 0.9998, 1.0000, 1.0000, 0.9990],\n",
       "        [0.9924, 0.9969, 0.9987, 0.9997, 0.9908],\n",
       "        [0.9844, 0.9937, 0.9973, 0.9988, 0.9813],\n",
       "        [0.9966, 0.9984, 0.9992, 0.9999, 0.9961],\n",
       "        [0.9928, 0.9950, 0.9944, 0.9995, 0.9935],\n",
       "        [0.9909, 0.9959, 0.9980, 0.9995, 0.9896],\n",
       "        [0.9970, 0.9985, 0.9993, 0.9999, 0.9965],\n",
       "        [0.9985, 0.9994, 0.9998, 1.0000, 0.9981],\n",
       "        [0.9948, 0.9977, 0.9990, 0.9998, 0.9938],\n",
       "        [0.9930, 0.9972, 0.9990, 0.9997, 0.9913]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = y + torch.randn_like(y)\n",
    "y_pred = model(x2)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Word Window Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our raw data, which consists of sentences\n",
    "corpus = [\n",
    "          \"We always come to Paris\",\n",
    "          \"The professor is from Australia\",\n",
    "          \"I live in Stanford\",\n",
    "          \"He comes from Taiwan\",\n",
    "          \"The capital of Turkey is Ankara\"\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['we', 'always', 'come', 'to', 'paris'],\n",
       " ['the', 'professor', 'is', 'from', 'australia'],\n",
       " ['i', 'live', 'in', 'stanford'],\n",
       " ['he', 'comes', 'from', 'taiwan'],\n",
       " ['the', 'capital', 'of', 'turkey', 'is', 'ankara']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    return sentence.lower().split()\n",
    "\n",
    "train_sentences = [sent.lower().split() for sent in corpus]\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1, 0, 1]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations = set([\"australia\", \"ankara\", \"paris\", \"stanford\", \"taiwan\", \"turkey\"])\n",
    "\n",
    "train_labels = [[1 if word in locations else 0 for word in sent] for sent in train_sentences]\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the words to embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'always',\n",
       " 'ankara',\n",
       " 'australia',\n",
       " 'capital',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'from',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'live',\n",
       " 'of',\n",
       " 'paris',\n",
       " 'professor',\n",
       " 'stanford',\n",
       " 'taiwan',\n",
       " 'the',\n",
       " 'to',\n",
       " 'turkey',\n",
       " 'we'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = set(w for s in train_sentences for w in s)\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary.add(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<pad>', 'we', 'always', 'come', 'to', 'paris', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.add(\"<pad>\")\n",
    "\n",
    "def pad_window(sentence, window_size, pad_token=\"<pad>\"):\n",
    "    window = [pad_token] * window_size\n",
    "    return window + sentence + window\n",
    "\n",
    "window_size = 2\n",
    "pad_window(train_sentences[0], window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<unk>': 1,\n",
       " 'always': 2,\n",
       " 'ankara': 3,\n",
       " 'australia': 4,\n",
       " 'capital': 5,\n",
       " 'come': 6,\n",
       " 'comes': 7,\n",
       " 'from': 8,\n",
       " 'he': 9,\n",
       " 'i': 10,\n",
       " 'in': 11,\n",
       " 'is': 12,\n",
       " 'live': 13,\n",
       " 'of': 14,\n",
       " 'paris': 15,\n",
       " 'professor': 16,\n",
       " 'stanford': 17,\n",
       " 'taiwan': 18,\n",
       " 'the': 19,\n",
       " 'to': 20,\n",
       " 'turkey': 21,\n",
       " 'we': 22}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix_to_word = sorted(list(vocabulary))\n",
    "\n",
    "word_to_ix = {word: ind for ind, word in enumerate(ix_to_word)}\n",
    "word_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_token_to_indices(sentence, word_to_ix):\n",
    "    indices = []\n",
    "    for token in sentence:\n",
    "        if token in word_to_ix:\n",
    "            index = word_to_ix[token]\n",
    "        else:\n",
    "            index = word_to_ix[\"<unk>\"]\n",
    "        indices.append(index)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[22, 2, 6, 20, 15],\n",
       " [19, 16, 12, 8, 4],\n",
       " [10, 13, 11, 17],\n",
       " [9, 7, 8, 18],\n",
       " [19, 5, 14, 21, 12, 3]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_padded_indices = [convert_token_to_indices(s, word_to_ix) for s in train_sentences]\n",
    "example_padded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-1.0438, -0.7594, -1.2429, -0.9675, -0.5421],\n",
       "         [ 0.5904, -0.3822, -0.3377, -0.5467, -1.6959],\n",
       "         [-0.1684,  0.0953,  0.4167, -0.9343, -0.7105],\n",
       "         [-0.7384,  0.8329, -1.3471, -0.7814, -0.5336],\n",
       "         [ 1.4964, -1.2145, -0.6583,  0.2651, -0.5711],\n",
       "         [-0.8416,  0.3527, -0.1740,  0.2533,  0.3015],\n",
       "         [ 1.3283,  0.0590,  0.0707,  0.7774, -0.5143],\n",
       "         [ 2.1485, -0.3947,  0.2431, -0.2072, -0.2714],\n",
       "         [ 0.9240, -1.2958, -0.4004,  2.0959, -0.6149],\n",
       "         [-0.1205,  1.7748,  0.2206, -0.9654, -0.8555],\n",
       "         [-0.8745, -0.3593,  0.5483, -1.1653,  0.2301],\n",
       "         [-1.3185,  0.2990,  1.3506, -0.8036, -0.1402],\n",
       "         [ 0.9432,  0.6465, -0.6482,  1.5425,  0.6648],\n",
       "         [-2.3618,  1.1250,  0.0108,  0.4851,  0.1148],\n",
       "         [-0.4075, -0.2102, -0.1306,  0.0360, -0.7633],\n",
       "         [ 0.1564, -0.3607,  1.1361,  0.8833, -1.4348],\n",
       "         [ 0.6241,  0.0524,  0.7946, -0.9099,  0.5626],\n",
       "         [-1.7290,  0.2263,  1.2940,  0.1274, -0.7746],\n",
       "         [ 1.5545, -1.0628,  0.6921,  0.1853, -0.3138],\n",
       "         [ 0.3579, -0.3503, -0.2148, -0.8738,  0.2535],\n",
       "         [ 0.3635,  0.3384, -0.4883, -0.2315,  0.5381],\n",
       "         [ 0.8314,  1.3269, -1.6609, -0.8367, -0.0646],\n",
       "         [-0.5067, -1.4754, -0.4420,  0.4902,  0.2470]], requires_grad=True)]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = 5\n",
    "embeds = nn.Embedding(len(vocabulary), embedding_dim)\n",
    "\n",
    "list(embeds.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1564, -0.3607,  1.1361,  0.8833, -1.4348],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the embedding for the word Paris\n",
    "index = word_to_ix[\"paris\"]\n",
    "index_tensor = torch.tensor(index, dtype=torch.long)\n",
    "paris_embed = embeds(index_tensor)\n",
    "paris_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1564, -0.3607,  1.1361,  0.8833, -1.4348],\n",
       "        [-0.7384,  0.8329, -1.3471, -0.7814, -0.5336]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_paris = word_to_ix[\"paris\"]\n",
    "index_ankara = word_to_ix[\"ankara\"]\n",
    "indices = [index_paris, index_ankara]\n",
    "indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "embeddings = embeds(indices_tensor)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "\n",
    "def _custom_collate_fn(batch, window_size, word_to_ix):\n",
    "  # Prepare the datapoints\n",
    "  x, y = zip(*batch)  \n",
    "  x = [pad_window(s, window_size=window_size) for s in x]\n",
    "  x = [convert_tokens_to_indices(s, word_to_ix) for s in x]\n",
    "\n",
    "  # Pad x so that all the examples in the batch have the same size\n",
    "  pad_token_ix = word_to_ix[\"<pad>\"]\n",
    "  x = [torch.LongTensor(x_i) for x_i in x]\n",
    "  x_padded = nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=pad_token_ix)\n",
    "\n",
    "  # Pad y and record the length\n",
    "  lengths = [len(label) for label in y]\n",
    "  lenghts = torch.LongTensor(lengths)\n",
    "  y = [torch.LongTensor(y_i) for y_i in y]\n",
    "  y_padded = nn.utils.rnn.pad_sequence(y, batch_first=True, padding_value=0)\n",
    "\n",
    "  return x_padded, y_padded, lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Batched Input:\n",
      "tensor([[ 0,  0,  9,  7,  8, 18,  0,  0,  0,  0],\n",
      "        [ 0,  0, 19,  5, 14, 21, 12,  3,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4, 6])\n",
      "\n",
      "Iteration 1\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 19, 16, 12,  8,  4,  0,  0],\n",
      "        [ 0,  0, 22,  2,  6, 20, 15,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([5, 5])\n",
      "\n",
      "Iteration 2\n",
      "Batched Input:\n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0]])\n",
      "Batched Labels:\n",
      "tensor([[0, 0, 0, 1]])\n",
      "Batched Lengths:\n",
      "tensor([4])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters to be passed to the DataLoader\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate the DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Go through one loop\n",
    "counter = 0\n",
    "for batched_x, batched_y, batched_lengths in loader:\n",
    "  print(f\"Iteration {counter}\")\n",
    "  print(\"Batched Input:\")\n",
    "  print(batched_x)\n",
    "  print(\"Batched Labels:\")\n",
    "  print(batched_y)\n",
    "  print(\"Batched Lengths:\")\n",
    "  print(batched_lengths)\n",
    "  print(\"\")\n",
    "  counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor: \n",
      "tensor([[ 0,  0, 10, 13, 11, 17,  0,  0]])\n",
      "\n",
      "Windows: \n",
      "tensor([[[ 0,  0, 10, 13, 11],\n",
      "         [ 0, 10, 13, 11, 17],\n",
      "         [10, 13, 11, 17,  0],\n",
      "         [13, 11, 17,  0,  0]]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Tensor: \")\n",
    "print(batched_x)\n",
    "print(\"\")\n",
    "\n",
    "chunk = batched_x.unfold(1, window_size*2 + 1, 1)\n",
    "print(f\"Windows: \")\n",
    "print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordWindowClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, hyperparameters, vocab_size, pad_ix=0):\n",
    "    super(WordWindowClassifier, self).__init__()\n",
    "    \n",
    "    \"\"\" Instance variables \"\"\"\n",
    "    self.window_size = hyperparameters[\"window_size\"]\n",
    "    self.embed_dim = hyperparameters[\"embed_dim\"]\n",
    "    self.hidden_dim = hyperparameters[\"hidden_dim\"]\n",
    "    self.freeze_embeddings = hyperparameters[\"freeze_embeddings\"]\n",
    "\n",
    "    \"\"\" Embedding Layer \n",
    "    Takes in a tensor containing embedding indices, and returns the \n",
    "    corresponding embeddings. The output is of dim \n",
    "    (number_of_indices * embedding_dim).\n",
    "\n",
    "    If freeze_embeddings is True, set the embedding layer parameters to be\n",
    "    non-trainable. This is useful if we only want the parameters other than the\n",
    "    embeddings parameters to change. \n",
    "\n",
    "    \"\"\"\n",
    "    self.embeds = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_ix)\n",
    "    if self.freeze_embeddings:\n",
    "      self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "    \"\"\" Hidden Layer\n",
    "    \"\"\"\n",
    "    full_window_size = 2 * window_size + 1\n",
    "    self.hidden_layer = nn.Sequential(\n",
    "      nn.Linear(full_window_size * self.embed_dim, self.hidden_dim), \n",
    "      nn.Tanh()\n",
    "    )\n",
    "\n",
    "    \"\"\" Output Layer\n",
    "    \"\"\"\n",
    "    self.output_layer = nn.Linear(self.hidden_dim, 1)\n",
    "\n",
    "    \"\"\" Probabilities \n",
    "    \"\"\"\n",
    "    self.probabilities = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    \"\"\"\n",
    "    Let B:= batch_size\n",
    "        L:= window-padded sentence length\n",
    "        D:= self.embed_dim\n",
    "        S:= self.window_size\n",
    "        H:= self.hidden_dim\n",
    "        \n",
    "    inputs: a (B, L) tensor of token indices\n",
    "    \"\"\"\n",
    "    B, L = inputs.size()\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L) LongTensor\n",
    "    Outputs a (B, L~, S) LongTensor\n",
    "    \"\"\"\n",
    "    # Fist, get our word windows for each word in our input.\n",
    "    token_windows = inputs.unfold(1, 2 * self.window_size + 1, 1)\n",
    "    _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "    # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "    assert token_windows.size() == (B, adjusted_length, 2 * self.window_size + 1)\n",
    "\n",
    "    \"\"\"\n",
    "    Embedding.\n",
    "    Takes in a torch.LongTensor of size (B, L~, S) \n",
    "    Outputs a (B, L~, S, D) FloatTensor.\n",
    "    \"\"\"\n",
    "    embedded_windows = self.embeds(token_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Reshaping.\n",
    "    Takes in a (B, L~, S, D) FloatTensor.\n",
    "    Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "    -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "    \"\"\"\n",
    "    embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 1.\n",
    "    Takes in a (B, L~, S*D) FloatTensor.\n",
    "    Resizes it into a (B, L~, H) FloatTensor\n",
    "    \"\"\"\n",
    "    layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "    \"\"\"\n",
    "    Layer 2\n",
    "    Takes in a (B, L~, H) FloatTensor.\n",
    "    Resizes it into a (B, L~, 1) FloatTensor.\n",
    "    \"\"\"\n",
    "    output = self.output_layer(layer_1)\n",
    "\n",
    "    \"\"\"\n",
    "    Softmax.\n",
    "    Takes in a (B, L~, 1) FloatTensor of unnormalized class scores.\n",
    "    Outputs a (B, L~, 1) FloatTensor of (log-)normalized class scores.\n",
    "    \"\"\"\n",
    "    output = self.probabilities(output)\n",
    "    output = output.view(B, -1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "data = list(zip(train_sentences, train_labels))\n",
    "batch_size = 2\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=window_size, word_to_ix=word_to_ix)\n",
    "\n",
    "# Instantiate a DataLoader\n",
    "loader = DataLoader(data, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn)\n",
    "\n",
    "# Initialize a model\n",
    "# It is useful to put all the model hyperparameters in a dictionary\n",
    "model_hyperparameters = {\n",
    "    \"batch_size\": 4,\n",
    "    \"window_size\": 2,\n",
    "    \"embed_dim\": 25,\n",
    "    \"hidden_dim\": 25,\n",
    "    \"freeze_embeddings\": False,\n",
    "}\n",
    "\n",
    "vocab_size = len(word_to_ix)\n",
    "model = WordWindowClassifier(model_hyperparameters, vocab_size)\n",
    "\n",
    "# Define an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define a loss function, which computes to binary cross entropy loss\n",
    "def loss_function(batch_outputs, batch_labels, batch_lengths):   \n",
    "    # Calculate the loss for the whole batch\n",
    "    bceloss = nn.BCELoss()\n",
    "    loss = bceloss(batch_outputs, batch_labels.float())\n",
    "\n",
    "    # Rescale the loss. Remember that we have used lengths to store the \n",
    "    # number of words in each training example\n",
    "    loss = loss / batch_lengths.sum().float()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that will be called in every epoch\n",
    "def train_epoch(loss_function, optimizer, model, loader):\n",
    "  \n",
    "  # Keep track of the total loss for the batch\n",
    "  total_loss = 0\n",
    "  for batch_inputs, batch_labels, batch_lengths in loader:\n",
    "    # Clear the gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Run a forward pass\n",
    "    outputs = model.forward(batch_inputs)\n",
    "    # Compute the batch loss\n",
    "    loss = loss_function(outputs, batch_labels, batch_lengths)\n",
    "    # Calculate the gradients\n",
    "    loss.backward()\n",
    "    # Update the parameteres\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  return total_loss\n",
    "\n",
    "\n",
    "# Function containing our main training loop\n",
    "def train(loss_function, optimizer, model, loader, num_epochs=10000):\n",
    "\n",
    "  # Iterate through each epoch and call our train_epoch function\n",
    "  for epoch in range(num_epochs):\n",
    "    epoch_loss = train_epoch(loss_function, optimizer, model, loader)\n",
    "    if epoch % 100 == 0: print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3036842495203018\n",
      "0.25503042340278625\n",
      "0.1798364520072937\n",
      "0.1650073491036892\n",
      "0.13519935123622417\n",
      "0.11038812808692455\n",
      "0.08957967069000006\n",
      "0.07133886963129044\n",
      "0.05756521504372358\n",
      "0.05000739265233278\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "train(loss_function, optimizer, model, loader, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = [\"She comes from Paris\"]\n",
    "test_sentences = [s.lower().split() for s in test_corpus]\n",
    "test_labels = [[0, 0, 0, 1]]\n",
    "\n",
    "test_data = list(zip(test_sentences, test_labels))\n",
    "batch_size = 1\n",
    "shuffle = True\n",
    "window_size = 2\n",
    "collate_fn = partial(custom_collate_fn, window_size=2, word_to_ix=word_to_ix)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 1]])\n",
      "tensor([[0.1545, 0.1098, 0.0938, 0.7376]], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "for test_instance, labels, _ in test_loader:\n",
    "    outputs = model.forward(test_instance)\n",
    "    print(labels)\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
